# Story 3.6: Data Integrity Validation & Backup

## Status
Draft

## Story
**As someone** relying on the dashboard for cognitive offload,
**I want** assurance that my data is never lost or corrupted during sync operations,
**so that** I can trust the dashboard as my "external brain" without fear of losing critical information.

## Acceptance Criteria
1. Sync operations include integrity checks: verify task counts before/after sync, log discrepancies
2. Database constraints prevent orphaned records: tasks cannot exist without parent project/phase (foreign key constraints with ON DELETE CASCADE)
3. Unique constraints prevent duplicate tasks during sync race conditions
4. Automated daily backup of Supabase database to external storage (GitHub repo or cloud storage) with 30-day retention
5. Backup restoration procedure documented and tested: can restore from backup within 1 hour if needed
6. Sync conflict detection: if same task modified simultaneously in multiple locations, conflict is detected and user is prompted to resolve
7. Data validation on all input fields: task titles (max 200 chars), dates (valid format), percentages (0-100 range)
8. Corrupted data detection script runs weekly: checks for null required fields, invalid foreign keys, orphaned records
9. Point-in-time recovery capability using Supabase backups: can restore to any point within last 7 days if data corruption occurs
10. Manual backup button in dashboard settings allows on-demand full data export to JSON file

## Tasks / Subtasks
- [ ] Task 1: Implement sync integrity checks (AC: 1)
  - [ ] Create src/utils/syncIntegrity.ts utility
  - [ ] Function: preSyncSnapshot() → captures task count, project count, phase count
  - [ ] Function: postSyncVerification(preSnapshot, postSnapshot) → compares counts
  - [ ] Log discrepancies: if post-sync count != pre-sync count + expected changes, log warning
  - [ ] Create integrity_logs table: id, sync_timestamp, operation, expected_count, actual_count, discrepancy, details
  - [ ] Integrate into useRealtimeSync hook: wrap sync operations with integrity checks
  - [ ] Display warning to user if discrepancy detected: "Sync may have issues. Check integrity log."
  - [ ] Weekly report: summarize integrity check results, flag anomalies
  - [ ] Alert on critical discrepancies: missing tasks, unexpected deletions
- [ ] Task 2: Add database foreign key constraints (AC: 2)
  - [ ] Review existing schema: verify foreign keys on tasks table
  - [ ] Ensure: tasks.project_id REFERENCES projects(id) ON DELETE CASCADE
  - [ ] Ensure: tasks.phase_id REFERENCES phases(id) ON DELETE CASCADE
  - [ ] Ensure: tasks.business_id REFERENCES businesses(id) ON DELETE CASCADE
  - [ ] Ensure: projects.business_id REFERENCES businesses(id) ON DELETE CASCADE
  - [ ] Ensure: phases.project_id REFERENCES projects(id) ON DELETE CASCADE
  - [ ] Create migration: .ai/migrations/add-foreign-key-constraints.sql
  - [ ] Test cascade: delete project → verify all tasks and phases deleted
  - [ ] Test orphan prevention: attempt to create task with non-existent project_id → verify fails
  - [ ] Document cascade behavior: deleting business deletes all projects, phases, tasks
- [ ] Task 3: Add unique constraints to prevent duplicates (AC: 3)
  - [ ] Identify unique constraint candidates: (user_id, title, project_id, phase_id) for tasks
  - [ ] Create unique index: `CREATE UNIQUE INDEX idx_tasks_unique ON tasks(user_id, title, project_id, phase_id) WHERE deleted_at IS NULL;`
  - [ ] Consider: partial unique index (excludes soft-deleted records)
  - [ ] Prevent race condition: two sync operations creating same task simultaneously
  - [ ] Alternative: use UPSERT logic with conflict resolution: `ON CONFLICT (unique_fields) DO UPDATE`
  - [ ] Test: create task "Write report" twice in rapid succession → verify second fails or updates first
  - [ ] Add user-friendly error message: "Task already exists. Did you mean to update it?"
  - [ ] Apply unique constraints to other tables: projects (user_id, name, business_id), businesses (user_id, name)
- [ ] Task 4: Set up automated daily database backup (AC: 4)
  - [ ] Supabase provides automatic backups (check Dashboard → Database → Backups)
  - [ ] Verify daily backups enabled in Supabase (may require Pro plan)
  - [ ] Alternative: create custom backup script using Supabase Management API
  - [ ] Create .ai/scripts/backup-database.sh script
  - [ ] Use pg_dump via Supabase connection string (if direct access available)
  - [ ] Alternative: export via Supabase API: query all tables, export to JSON
  - [ ] Store backups in: GitHub repo (private, .ai/backups/), AWS S3, or Google Drive
  - [ ] Backup naming: `backup-YYYY-MM-DD.sql` or `backup-YYYY-MM-DD.json`
  - [ ] Implement 30-day retention: delete backups older than 30 days
  - [ ] Schedule: GitHub Actions cron job runs daily at 3 AM UTC
  - [ ] Encrypt backups before storage (contains sensitive client data)
- [ ] Task 5: Document backup restoration procedure (AC: 5)
  - [ ] Create .ai/backup-restoration-guide.md document
  - [ ] Step 1: Locate backup file (GitHub repo or cloud storage)
  - [ ] Step 2: Verify backup integrity (checksum, file size)
  - [ ] Step 3: For SQL backup: connect to Supabase, run psql restore command
  - [ ] Step 4: For JSON backup: use Supabase client to re-insert data
  - [ ] Step 5: Verify restoration: check record counts, sample data
  - [ ] Step 6: Test application: login, access tasks, verify functionality
  - [ ] Include troubleshooting: common errors and solutions
  - [ ] Target: complete restoration in under 1 hour
  - [ ] Test restoration procedure annually to ensure it works
  - [ ] Document contact info: Supabase support if restoration fails
- [ ] Task 6: Implement sync conflict detection (AC: 6)
  - [ ] Add version field to tasks table: version INTEGER DEFAULT 1
  - [ ] Increment version on every update: `UPDATE tasks SET version = version + 1 WHERE id = ?`
  - [ ] On sync: check if local version matches server version
  - [ ] If mismatch: conflict detected (task modified elsewhere)
  - [ ] Display conflict resolution UI: show both versions side-by-side
  - [ ] User chooses: "Keep local", "Keep server", or "Merge changes"
  - [ ] Log conflicts to conflicts_log table: id, task_id, local_version, server_version, resolved_at, resolution_method
  - [ ] Alternative: use updated_at timestamp comparison (simpler but less robust)
  - [ ] Test: modify task in Tab 1, modify same task in Tab 2, sync both → verify conflict detected
  - [ ] Add conflict indicator badge: "⚠️ Conflict detected - resolve now"
- [ ] Task 7: Add comprehensive input validation (AC: 7)
  - [ ] Create src/utils/validation.ts utility
  - [ ] Validate task title: required, max 200 chars, trim whitespace
  - [ ] Validate dates: valid ISO format, not in distant past (>1 year), reasonable future (<5 years)
  - [ ] Validate percentages: 0-100 range, integer only
  - [ ] Validate emails: regex pattern for valid email format
  - [ ] Validate URLs: valid HTTP/HTTPS format (for project links)
  - [ ] Validate currency amounts: positive numbers, max 2 decimal places
  - [ ] Database-level constraints: `CHECK (progress_percentage >= 0 AND progress_percentage <= 100)`
  - [ ] Client-side validation: display errors before submission
  - [ ] Server-side validation: Supabase RPC functions or database triggers
  - [ ] User-friendly error messages: "Task title must be between 1 and 200 characters"
- [ ] Task 8: Create corrupted data detection script (AC: 8)
  - [ ] Create .ai/scripts/detect-corrupted-data.sql script
  - [ ] Check 1: Tasks with null required fields: `SELECT * FROM tasks WHERE title IS NULL OR user_id IS NULL`
  - [ ] Check 2: Orphaned tasks (invalid project_id): `SELECT * FROM tasks WHERE project_id NOT IN (SELECT id FROM projects)`
  - [ ] Check 3: Invalid date ranges: `SELECT * FROM tasks WHERE due_date < created_at`
  - [ ] Check 4: Invalid percentages: `SELECT * FROM tasks WHERE progress_percentage < 0 OR progress_percentage > 100`
  - [ ] Check 5: Duplicate tasks (violating uniqueness): `SELECT title, COUNT(*) FROM tasks GROUP BY title HAVING COUNT(*) > 1`
  - [ ] Check 6: Projects without business: `SELECT * FROM projects WHERE business_id IS NULL`
  - [ ] Generate report: table name, issue type, affected records count, sample IDs
  - [ ] Schedule: GitHub Actions cron job runs weekly (Sunday 2 AM)
  - [ ] Email notification if issues found: "Data integrity issues detected - review required"
  - [ ] Auto-fix option: script can automatically repair certain issues (null dates → set to today)
- [ ] Task 9: Configure Supabase Point-in-Time Recovery (AC: 9)
  - [ ] Verify Supabase plan supports point-in-time recovery (PITR)
  - [ ] Supabase Pro plan includes PITR for last 7 days
  - [ ] Navigate to Supabase Dashboard → Database → Backups → Point in Time Recovery
  - [ ] Test PITR: create test data, note timestamp, delete data, restore to timestamp
  - [ ] Document PITR procedure in .ai/backup-restoration-guide.md
  - [ ] PITR steps: Dashboard → Database → Backups → "Restore to point in time" → select timestamp
  - [ ] Warning: PITR creates new database, requires connection string update
  - [ ] Downtime: 10-30 minutes for PITR restoration
  - [ ] Alternative: use daily backups for restore (longer recovery time but more control)
  - [ ] Include PITR in disaster recovery plan
- [ ] Task 10: Build manual backup/export feature (AC: 10)
  - [ ] Add "Backup & Export" section to Settings page
  - [ ] Create src/utils/dataBackup.ts utility
  - [ ] Function: exportAllData() → queries all tables, returns JSON
  - [ ] Include tables: tasks, projects, phases, businesses, daily_pages, deep_work_sessions, health_goals, content_items, finance_records, life_items, golf_scores
  - [ ] Exclude: audit_logs, login_attempts (too large, not critical for restore)
  - [ ] JSON structure: `{ version: "1.0", exported_at: "timestamp", user_id: "uuid", tables: { tasks: [...], projects: [...] } }`
  - [ ] Add "Backup Now" button in Settings → triggers export
  - [ ] Download JSON file: `dashboard-backup-YYYY-MM-DD.json`
  - [ ] Display success message: "Backup created successfully. File saved to Downloads."
  - [ ] Add "Import Backup" feature: upload JSON, restore data (with conflict resolution)
  - [ ] Watermark export: "Created by [user] on [date] - Handle securely, contains confidential data"

## Dev Notes

### Previous Story Insights
**From Story 3.1:** [Source: docs/stories/3.1.pre-deployment-security-checklist.md]
- Security checklist includes data integrity verification
- Pre-deployment checks ensure database constraints in place

**From Story 3.2:** [Source: docs/stories/3.2.github-secret-scanning-protection.md]
- GitHub Actions workflows established for automation
- Can use for scheduled backup jobs

**From Story 3.3:** [Source: docs/stories/3.3.supabase-rls-policies.md]
- RLS policies protect data access
- Foreign key relationships established between tables

**From Story 3.4:** [Source: docs/stories/3.4.client-data-encryption-protocols.md]
- Audit logging infrastructure exists
- Data retention and cleanup procedures defined

**From Story 3.5:** [Source: docs/stories/3.5.authentication-session-security.md]
- Authentication provides user_id for data isolation
- Session tracking ensures user context

**From Story 1.1:** [Source: docs/stories/1.1.tasks-hub-page-structure.md]
- Database schema: tasks, projects, phases, businesses, life_areas
- Foreign key relationships mentioned but may need strengthening
- Real-time sync via useRealtimeSync hook

**From Story 1.2-1.5:** [Source: docs/stories/1.2-1.5]
- Bidirectional sync established between Tasks Hub and other pages
- Sync latency <500ms requirement
- Optimistic updates pattern used

**Key Context:**
- Final story in Epic 3 - ensures data reliability across security foundation
- Dashboard is "external brain" = data loss is catastrophic
- Sync operations from multiple tabs/devices = risk of conflicts and race conditions
- Need bulletproof data integrity + disaster recovery capability
- Backups must include ALL tables (not just tasks)

### Architecture Context

**Tech Stack:** [Source: docs/prd/technical-assumptions.md]
- Backend: Supabase PostgreSQL (supports foreign keys, constraints, triggers)
- Real-time: Supabase real-time subscriptions (used in sync)
- State: React Query with optimistic updates
- Backup: GitHub Actions for scheduled jobs, Supabase backup features

**Database Constraint Hierarchy:**
```
businesses (top level)
  ↓ projects (FK: business_id)
    ↓ phases (FK: project_id)
      ↓ tasks (FK: phase_id, project_id, business_id)

Cascade rules:
- Delete business → cascade delete projects, phases, tasks
- Delete project → cascade delete phases, tasks
- Delete phase → cascade delete tasks (only those in that phase)
```

**Foreign Key Constraints Pattern:**
```sql
-- .ai/migrations/add-foreign-key-constraints.sql

-- Tasks table foreign keys with CASCADE
ALTER TABLE tasks
  DROP CONSTRAINT IF EXISTS tasks_business_id_fkey,
  ADD CONSTRAINT tasks_business_id_fkey
    FOREIGN KEY (business_id)
    REFERENCES businesses(id)
    ON DELETE CASCADE;

ALTER TABLE tasks
  DROP CONSTRAINT IF EXISTS tasks_project_id_fkey,
  ADD CONSTRAINT tasks_project_id_fkey
    FOREIGN KEY (project_id)
    REFERENCES projects(id)
    ON DELETE CASCADE;

ALTER TABLE tasks
  DROP CONSTRAINT IF EXISTS tasks_phase_id_fkey,
  ADD CONSTRAINT tasks_phase_id_fkey
    FOREIGN KEY (phase_id)
    REFERENCES phases(id)
    ON DELETE CASCADE;

-- Projects table foreign keys
ALTER TABLE projects
  DROP CONSTRAINT IF EXISTS projects_business_id_fkey,
  ADD CONSTRAINT projects_business_id_fkey
    FOREIGN KEY (business_id)
    REFERENCES businesses(id)
    ON DELETE CASCADE;

-- Phases table foreign keys
ALTER TABLE phases
  DROP CONSTRAINT IF EXISTS phases_project_id_fkey,
  ADD CONSTRAINT phases_project_id_fkey
    FOREIGN KEY (project_id)
    REFERENCES projects(id)
    ON DELETE CASCADE;

-- Verify constraints
SELECT
  tc.table_name,
  tc.constraint_name,
  kcu.column_name,
  ccu.table_name AS foreign_table_name,
  ccu.column_name AS foreign_column_name,
  rc.delete_rule
FROM information_schema.table_constraints AS tc
JOIN information_schema.key_column_usage AS kcu
  ON tc.constraint_name = kcu.constraint_name
JOIN information_schema.constraint_column_usage AS ccu
  ON ccu.constraint_name = tc.constraint_name
JOIN information_schema.referential_constraints AS rc
  ON rc.constraint_name = tc.constraint_name
WHERE tc.constraint_type = 'FOREIGN KEY'
  AND tc.table_name IN ('tasks', 'projects', 'phases')
ORDER BY tc.table_name, kcu.column_name;
```

**Unique Constraints Pattern:**
```sql
-- Prevent duplicate tasks (same title in same project/phase)
CREATE UNIQUE INDEX idx_tasks_unique
  ON tasks(user_id, title, project_id, phase_id)
  WHERE deleted_at IS NULL; -- Exclude soft-deleted records

-- Prevent duplicate projects (same name in same business)
CREATE UNIQUE INDEX idx_projects_unique
  ON projects(user_id, name, business_id)
  WHERE deleted_at IS NULL;

-- Prevent duplicate businesses
CREATE UNIQUE INDEX idx_businesses_unique
  ON businesses(user_id, name)
  WHERE deleted_at IS NULL;

-- Test uniqueness
INSERT INTO tasks (user_id, title, project_id, phase_id, ...)
VALUES (...) -- First insert succeeds

INSERT INTO tasks (user_id, title, project_id, phase_id, ...)
VALUES (...) -- Second insert with same values fails with unique constraint violation
```

**Sync Integrity Check Pattern:**
```typescript
// src/utils/syncIntegrity.ts
import { supabase } from '@/lib/supabase';

interface SyncSnapshot {
  taskCount: number;
  projectCount: number;
  phaseCount: number;
  timestamp: Date;
}

export async function capturePreSyncSnapshot(): Promise<SyncSnapshot> {
  const { count: taskCount } = await supabase
    .from('tasks')
    .select('*', { count: 'exact', head: true });

  const { count: projectCount } = await supabase
    .from('projects')
    .select('*', { count: 'exact', head: true });

  const { count: phaseCount } = await supabase
    .from('phases')
    .select('*', { count: 'exact', head: true });

  return {
    taskCount: taskCount || 0,
    projectCount: projectCount || 0,
    phaseCount: phaseCount || 0,
    timestamp: new Date(),
  };
}

export async function verifyPostSync(
  preSnapshot: SyncSnapshot,
  expectedChanges: { tasks?: number; projects?: number; phases?: number }
): Promise<{ isValid: boolean; discrepancies: string[] }> {
  const postSnapshot = await capturePreSyncSnapshot();

  const discrepancies: string[] = [];

  // Check tasks
  const expectedTaskCount = preSnapshot.taskCount + (expectedChanges.tasks || 0);
  if (postSnapshot.taskCount !== expectedTaskCount) {
    discrepancies.push(
      `Task count mismatch: expected ${expectedTaskCount}, got ${postSnapshot.taskCount}`
    );
  }

  // Check projects
  const expectedProjectCount = preSnapshot.projectCount + (expectedChanges.projects || 0);
  if (postSnapshot.projectCount !== expectedProjectCount) {
    discrepancies.push(
      `Project count mismatch: expected ${expectedProjectCount}, got ${postSnapshot.projectCount}`
    );
  }

  // Check phases
  const expectedPhaseCount = preSnapshot.phaseCount + (expectedChanges.phases || 0);
  if (postSnapshot.phaseCount !== expectedPhaseCount) {
    discrepancies.push(
      `Phase count mismatch: expected ${expectedPhaseCount}, got ${postSnapshot.phaseCount}`
    );
  }

  if (discrepancies.length > 0) {
    // Log to integrity_logs table
    await supabase.from('integrity_logs').insert({
      sync_timestamp: preSnapshot.timestamp.toISOString(),
      operation: 'sync_verification',
      expected_count: expectedTaskCount,
      actual_count: postSnapshot.taskCount,
      discrepancy: discrepancies.join('; '),
    });
  }

  return {
    isValid: discrepancies.length === 0,
    discrepancies,
  };
}
```

**Conflict Detection Pattern:**
```typescript
// Add version field to tasks table
// ALTER TABLE tasks ADD COLUMN version INTEGER DEFAULT 1;

// On update, check version
export async function updateTaskWithConflictDetection(
  taskId: string,
  updates: Partial<Task>,
  localVersion: number
): Promise<{ success: boolean; conflict?: boolean; serverVersion?: number }> {
  // Get current server version
  const { data: currentTask } = await supabase
    .from('tasks')
    .select('version')
    .eq('id', taskId)
    .single();

  if (!currentTask) {
    return { success: false };
  }

  if (currentTask.version !== localVersion) {
    // Conflict detected!
    return {
      success: false,
      conflict: true,
      serverVersion: currentTask.version,
    };
  }

  // No conflict, proceed with update
  const { error } = await supabase
    .from('tasks')
    .update({
      ...updates,
      version: localVersion + 1,
    })
    .eq('id', taskId)
    .eq('version', localVersion); // Optimistic locking

  return { success: !error };
}
```

**Automated Backup Script:**
```bash
#!/bin/bash
# .ai/scripts/backup-database.sh

set -e

BACKUP_DIR=".ai/backups"
BACKUP_FILE="backup-$(date +%Y-%m-%d).json"
RETENTION_DAYS=30

echo "Starting database backup..."

# Export all tables to JSON using Supabase API
# Requires SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY in environment

node .ai/scripts/export-database.js > "$BACKUP_DIR/$BACKUP_FILE"

# Encrypt backup (contains sensitive data)
gpg --symmetric --cipher-algo AES256 "$BACKUP_DIR/$BACKUP_FILE"
rm "$BACKUP_DIR/$BACKUP_FILE" # Remove unencrypted version

echo "Backup created: $BACKUP_DIR/$BACKUP_FILE.gpg"

# Delete backups older than 30 days
find "$BACKUP_DIR" -name "backup-*.json.gpg" -mtime +$RETENTION_DAYS -delete

echo "Old backups cleaned up (retention: $RETENTION_DAYS days)"
```

**Export Database Script:**
```javascript
// .ai/scripts/export-database.js
const { createClient } = require('@supabase/supabase-js');

const supabaseUrl = process.env.VITE_SUPABASE_URL;
const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;

const supabase = createClient(supabaseUrl, supabaseKey);

async function exportAllData() {
  const tables = [
    'tasks', 'projects', 'phases', 'businesses', 'life_areas',
    'daily_pages', 'deep_work_sessions', 'health_goals',
    'content_items', 'finance_records', 'life_items', 'golf_scores',
  ];

  const backup = {
    version: '1.0',
    exported_at: new Date().toISOString(),
    tables: {},
  };

  for (const table of tables) {
    const { data, error } = await supabase.from(table).select('*');
    if (error) {
      console.error(`Error exporting ${table}:`, error);
      process.exit(1);
    }
    backup.tables[table] = data;
  }

  console.log(JSON.stringify(backup, null, 2));
}

exportAllData();
```

**GitHub Actions Backup Workflow:**
```yaml
# .github/workflows/daily-backup.yml
name: Daily Database Backup

on:
  schedule:
    - cron: '0 3 * * *' # Daily at 3 AM UTC
  workflow_dispatch: # Allow manual trigger

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm install @supabase/supabase-js

      - name: Run backup script
        env:
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          GPG_PASSPHRASE: ${{ secrets.BACKUP_ENCRYPTION_KEY }}
        run: |
          mkdir -p .ai/backups
          chmod +x .ai/scripts/backup-database.sh
          .ai/scripts/backup-database.sh

      - name: Upload backup to artifact
        uses: actions/upload-artifact@v4
        with:
          name: database-backup-${{ github.run_number }}
          path: .ai/backups/*.json.gpg
          retention-days: 30

      - name: Commit backup to repository
        run: |
          git config user.name "Backup Bot"
          git config user.email "backup@github-actions"
          git add .ai/backups/*.json.gpg
          git commit -m "Automated backup $(date +%Y-%m-%d)" || echo "No changes"
          git push
```

**Corrupted Data Detection Script:**
```sql
-- .ai/scripts/detect-corrupted-data.sql

-- Create temporary report table
CREATE TEMP TABLE data_integrity_report (
  check_name TEXT,
  issue_count INTEGER,
  sample_ids TEXT[]
);

-- Check 1: Tasks with null required fields
INSERT INTO data_integrity_report
SELECT
  'tasks_null_required_fields',
  COUNT(*),
  ARRAY_AGG(id::TEXT)
FROM tasks
WHERE title IS NULL OR user_id IS NULL OR created_at IS NULL;

-- Check 2: Orphaned tasks (invalid project_id)
INSERT INTO data_integrity_report
SELECT
  'orphaned_tasks',
  COUNT(*),
  ARRAY_AGG(id::TEXT)
FROM tasks
WHERE project_id IS NOT NULL
  AND project_id NOT IN (SELECT id FROM projects);

-- Check 3: Invalid progress percentages
INSERT INTO data_integrity_report
SELECT
  'invalid_progress_percentage',
  COUNT(*),
  ARRAY_AGG(id::TEXT)
FROM tasks
WHERE progress_percentage < 0 OR progress_percentage > 100;

-- Check 4: Invalid date ranges
INSERT INTO data_integrity_report
SELECT
  'invalid_date_ranges',
  COUNT(*),
  ARRAY_AGG(id::TEXT)
FROM tasks
WHERE due_date < created_at;

-- Check 5: Duplicate tasks (potential race condition)
INSERT INTO data_integrity_report
SELECT
  'duplicate_tasks',
  COUNT(*),
  ARRAY_AGG(id::TEXT)
FROM (
  SELECT id, title, project_id, phase_id,
    COUNT(*) OVER (PARTITION BY title, project_id, phase_id) as dup_count
  FROM tasks
) t
WHERE dup_count > 1;

-- Generate report
SELECT
  check_name,
  issue_count,
  CASE
    WHEN issue_count = 0 THEN '✅ OK'
    WHEN issue_count < 10 THEN '⚠️ Minor issues'
    ELSE '❌ Critical issues'
  END as status,
  sample_ids
FROM data_integrity_report
ORDER BY issue_count DESC;
```

**File Locations:**
- Create: .ai/migrations/add-foreign-key-constraints.sql (database migration)
- Create: .ai/migrations/add-unique-constraints.sql (database migration)
- Create: .ai/migrations/add-version-column.sql (conflict detection)
- Create: .ai/scripts/backup-database.sh (backup script)
- Create: .ai/scripts/export-database.js (data export utility)
- Create: .ai/scripts/detect-corrupted-data.sql (integrity check)
- Create: .ai/backup-restoration-guide.md (restoration documentation)
- Create: .github/workflows/daily-backup.yml (automated backups)
- Create: .github/workflows/weekly-integrity-check.yml (corrupted data detection)
- Create: src/utils/syncIntegrity.ts (sync verification)
- Create: src/utils/conflictDetection.ts (conflict resolution)
- Create: src/utils/validation.ts (input validation)
- Create: src/utils/dataBackup.ts (manual backup/export)
- Create: src/components/settings/BackupExport.tsx (backup UI)
- Update: src/hooks/useRealtimeSync.ts (add integrity checks)
- Update: .ai/security-checklist.md (add data integrity verification)

### Testing

**Testing Requirements:** [Source: docs/prd/technical-assumptions.md]
Manual testing with systematic verification (no automated test suite per PRD)

**Data Integrity Validation Workflow:**

1. **Foreign Key Cascade Testing:**
   ```sql
   -- Create test business with projects and tasks
   INSERT INTO businesses (id, user_id, name) VALUES ('test-biz', auth.uid(), 'Test Co');
   INSERT INTO projects (id, user_id, name, business_id) VALUES ('test-proj', auth.uid(), 'Test Project', 'test-biz');
   INSERT INTO tasks (id, user_id, title, project_id) VALUES ('test-task', auth.uid(), 'Test Task', 'test-proj');

   -- Delete business, verify cascade
   DELETE FROM businesses WHERE id = 'test-biz';

   -- Verify project and task deleted
   SELECT * FROM projects WHERE id = 'test-proj'; -- Expected: 0 rows
   SELECT * FROM tasks WHERE id = 'test-task'; -- Expected: 0 rows
   ```

2. **Unique Constraint Testing:**
   ```sql
   -- Create task
   INSERT INTO tasks (user_id, title, project_id, phase_id) VALUES (auth.uid(), 'Write Report', 'proj-1', 'phase-1');

   -- Attempt duplicate
   INSERT INTO tasks (user_id, title, project_id, phase_id) VALUES (auth.uid(), 'Write Report', 'proj-1', 'phase-1');
   -- Expected: ERROR: duplicate key value violates unique constraint
   ```

3. **Sync Integrity Check Testing:**
   - Create 10 tasks manually
   - Capture pre-sync snapshot (count = 10)
   - Perform sync operation that adds 3 tasks
   - Capture post-sync snapshot
   - Verify: postCount = 13 (10 + 3)
   - Manually delete 1 task during sync (simulate error)
   - Verify: integrity check detects discrepancy, logs to integrity_logs

4. **Conflict Detection Testing:**
   - Open dashboard in Tab 1 and Tab 2
   - In Tab 1: edit task title to "Updated Title A"
   - In Tab 2: edit same task title to "Updated Title B"
   - Save in Tab 1 → succeeds, version incremented
   - Save in Tab 2 → conflict detected
   - Verify conflict resolution UI appears
   - Choose "Keep Tab 2 version" → verify overwrites Tab 1

5. **Input Validation Testing:**
   - Task title: enter 250-character string → verify error: "Max 200 characters"
   - Progress percentage: enter 150 → verify error: "Must be 0-100"
   - Due date: enter "invalid-date" → verify error: "Invalid date format"
   - Email: enter "not-an-email" → verify error: "Invalid email format"
   - Verify validation runs before submission (client-side)
   - Verify validation enforced on server (database constraints)

6. **Automated Backup Testing:**
   - Trigger GitHub Actions workflow manually (workflow_dispatch)
   - Wait for backup job to complete
   - Check Actions artifacts → verify backup-*.json.gpg file exists
   - Download backup, decrypt with GPG
   - Verify JSON contains all tables with data
   - Check .ai/backups/ directory → verify backup committed to repo
   - Wait 31 days (simulate) → verify old backups deleted

7. **Backup Restoration Testing:**
   - Create test data: 5 projects, 20 tasks
   - Export backup using manual export feature
   - Delete all test data from database
   - Follow restoration guide: .ai/backup-restoration-guide.md
   - Restore from backup JSON
   - Verify all 5 projects and 20 tasks restored correctly
   - Verify no data loss or corruption
   - Measure restoration time → ensure < 1 hour

8. **Corrupted Data Detection Testing:**
   ```sql
   -- Create intentionally corrupted data for testing
   -- (use service role key to bypass constraints)

   -- Null required field
   INSERT INTO tasks (id, user_id, title) VALUES (gen_random_uuid(), auth.uid(), NULL);

   -- Orphaned task (invalid project_id)
   INSERT INTO tasks (id, user_id, title, project_id) VALUES (gen_random_uuid(), auth.uid(), 'Orphan', 'non-existent-id');

   -- Invalid percentage
   UPDATE tasks SET progress_percentage = 150 WHERE id = 'some-id';

   -- Run detection script
   -- Expected: script detects all 3 issues, reports sample IDs

   -- Clean up test corruption
   ```

9. **Point-in-Time Recovery Testing:**
   - Create test project: "PITR Test Project"
   - Note timestamp: 2025-10-07 10:30:00
   - Wait 1 hour, delete project
   - Navigate to Supabase Dashboard → Database → Backups → Point in Time Recovery
   - Select timestamp: 2025-10-07 10:30:00
   - Initiate restore
   - Wait for new database creation (10-30 minutes)
   - Update connection string to new database
   - Verify "PITR Test Project" exists in restored database

10. **Manual Backup/Export Testing:**
    - Navigate to Settings → Backup & Export
    - Click "Backup Now" button
    - Wait for export to complete (progress indicator)
    - Verify download: dashboard-backup-YYYY-MM-DD.json
    - Open JSON file → verify all tables present
    - Verify watermark: "Created by [user] on [date] - Confidential"
    - Check audit_logs → verify export event logged
    - Test import: upload same JSON, verify prompts for conflict resolution

11. **End-to-End Data Integrity:**
    - Create complex data: business → projects → phases → tasks
    - Perform sync operations in multiple tabs simultaneously
    - Verify no duplicate tasks created (unique constraints work)
    - Delete business → verify cascade deletes all related data
    - Check integrity_logs → verify no discrepancies
    - Export backup → verify all data captured
    - Run corrupted data detection → verify all checks pass
    - Simulate data loss: delete all tasks
    - Restore from backup → verify full recovery
    - Check task counts before/after → verify match

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-07 | v1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record
### Agent Model Used
_To be populated by development agent_

### Debug Log References
_To be populated by development agent_

### Completion Notes List
_To be populated by development agent_

### File List
_To be populated by development agent_

## QA Results
_To be populated by QA agent_
